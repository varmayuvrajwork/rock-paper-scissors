# Rock-Paper-Scissors Plus: AI Judge

> **Assignment**: Applied AI Engineer (Conversational Agents) - upliance.ai  
> **Focus**: Prompt-driven AI decision making with minimal hardcoded logic

---

## üéØ Overview

This project implements an **AI Judge** for a Rock-Paper-Scissors Plus game where decision-making is driven by **prompt engineering** rather than hardcoded logic. The system uses Google Gemini with LangChain to interpret natural language inputs, validate moves against game constraints, and provide clear explanations for all decisions.

### Key Highlights

‚úÖ **Prompt-Driven Logic** - 80% of decision logic is in prompts, not code  
‚úÖ **Natural Language Understanding** - Accepts typos, synonyms, and free-text inputs  
‚úÖ **Constraint Enforcement via Prompting** - Bomb usage limitation enforced by LLM  
‚úÖ **Clean Three-Layer Architecture** - Intent ‚Üí Logic ‚Üí Response  
‚úÖ **Comprehensive Edge Case Handling** - 30+ test cases covering ambiguity, typos, synonyms  
‚úÖ **Two Interfaces** - CLI (primary) + FastAPI/Web UI (bonus)  

---

## üéÆ Project Objective

**From Assignment PDF:**

> Design a prompt-driven AI Judge that evaluates user inputs against a set of rules and gives structured decisions. Focus on:
> - Prompt quality
> - Instruction design
> - Edge-case handling
> - Explainability

**Game Rules:**
- Valid moves: `rock`, `paper`, `scissors`, `bomb`
- Standard RPS rules apply
- Bomb beats everything (can only be used ONCE)
- Bomb vs bomb ‚Üí draw
- Invalid/unclear moves waste the turn

---

## üöÄ Quick Start

### Prerequisites
- Python 3.8 or higher
- Google Gemini API key

### Installation

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set up API key
# Create .env file with:
GOOGLE_API_KEY=your_gemini_api_key_here

# 3. Run the CLI game (PRIMARY SUBMISSION)
python main.py

# OR run the FastAPI version (BONUS - Web UI)
python api.py
# Then open index.html in browser
```

### First Game

```bash
$ python main.py

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë   ROCK-PAPER-SCISSORS PLUS: AI JUDGE EDITION      ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Your move: rock
Your move: I choose paper
Your move: bomb
Your move: quit
```

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ main.py                 # CLI game orchestrator (PRIMARY)
‚îú‚îÄ‚îÄ api.py                  # FastAPI REST API (BONUS)
‚îú‚îÄ‚îÄ index.html              # Web interface (BONUS)
‚îÇ
‚îú‚îÄ‚îÄ judge.py                # Layer 1: Intent Understanding (LLM)
‚îú‚îÄ‚îÄ game_logic.py           # Layer 2: Game Mechanics (Minimal)
‚îú‚îÄ‚îÄ response.py             # Layer 3: Response Formatting
‚îÇ
‚îú‚îÄ‚îÄ models.py               # Pydantic models for type safety
‚îú‚îÄ‚îÄ prompts.py              # ‚≠ê THE KEY FILE - All prompts and rules
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ .env                    # Your API key (create this)
‚îî‚îÄ‚îÄ README.md              # This file
```

### File Descriptions

| File | Purpose | Lines of Code | Assignment Relevance |
|------|---------|---------------|---------------------|
| **prompts.py** | ‚≠ê System prompts, rules, edge cases | ~100 | **PRIMARY FOCUS** |
| **judge.py** | LLM integration, intent understanding | ~80 | Architecture Layer 1 |
| **game_logic.py** | Minimal game mechanics | ~150 | Architecture Layer 2 |
| **response.py** | Output formatting | ~100 | Architecture Layer 3 |
| **models.py** | Pydantic models for structured output | ~60 | State modeling |
| **main.py** | CLI orchestrator | ~130 | Required interface |
| **api.py** | REST API server | ~300 | Bonus feature |
| **index.html** | Web UI | ~400 | Bonus feature |

---

## üèóÔ∏è Architecture

### Three-Layer Design (As Required by Assignment)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    USER INPUT                           ‚îÇ
‚îÇ          "I want to play rock"  /  "rok"                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LAYER 1: INTENT UNDERSTANDING                   ‚îÇ
‚îÇ              (judge.py - LLM Driven)                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚Ä¢ Uses Google Gemini via LangChain                     ‚îÇ
‚îÇ  ‚Ä¢ Structured output with Pydantic                      ‚îÇ
‚îÇ  ‚Ä¢ Interprets free text ‚Üí VALID/INVALID/UNCLEAR         ‚îÇ
‚îÇ  ‚Ä¢ Provides reasoning for classification                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Input:  "rok"                                          ‚îÇ
‚îÇ  Output: {                                              ‚îÇ
‚îÇ    classification: "VALID",                             ‚îÇ
‚îÇ    interpreted_move: "rock",                            ‚îÇ
‚îÇ    reasoning: "Common typo for rock"                    ‚îÇ
‚îÇ  }                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            LAYER 2: GAME LOGIC                          ‚îÇ
‚îÇ          (game_logic.py - Minimal Python)               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚Ä¢ Generate bot move                                    ‚îÇ
‚îÇ  ‚Ä¢ Determine winner (rock beats scissors, etc.)         ‚îÇ
‚îÇ  ‚Ä¢ Update scores and state                              ‚îÇ
‚îÇ  ‚Ä¢ Track bomb usage                                     ‚îÇ
‚îÇ  ‚Ä¢ NO decision logic - just mechanics                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Input:  validated_move="rock", bot_move="scissors"     ‚îÇ
‚îÇ  Output: {                                              ‚îÇ
‚îÇ    winner: "user",                                      ‚îÇ
‚îÇ    explanation: "Rock crushes scissors"                 ‚îÇ
‚îÇ  }                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LAYER 3: RESPONSE GENERATION                    ‚îÇ
‚îÇ            (response.py - Formatting)                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚Ä¢ Format round results for display                     ‚îÇ
‚îÇ  ‚Ä¢ Show round number, moves, winner                     ‚îÇ
‚îÇ  ‚Ä¢ Display running score                                ‚îÇ
‚îÇ  ‚Ä¢ Generate final game summary                          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Input:  round_result                                   ‚îÇ
‚îÇ  Output: Formatted text for console                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   USER OUTPUT                           ‚îÇ
‚îÇ  "Round 1: You played ROCK, Bot played SCISSORS"        ‚îÇ
‚îÇ  "‚Üí You win! Rock crushes scissors."                    ‚îÇ
‚îÇ  "Score: You 1 - Bot 0"                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why This Architecture?

1. **Separation of Concerns**: Each layer has ONE job
   - Layer 1: "What did the user mean?"
   - Layer 2: "What happens in the game?"
   - Layer 3: "How do we show it?"

2. **Testable**: Each layer can be tested independently
3. **Maintainable**: Changes in one layer don't affect others
4. **Extensible**: Easy to add new features or swap components

---

## üí° Prompt Design Philosophy

### Core Principle: Logic in Prompts, Not Code

**Traditional Approach** (NOT used here):
```python
# ‚ùå Hardcoded logic
if user_input.lower() in ["rock", "rok", "stone"]:
    move = "rock"
elif user_input.lower() == "rock and paper":
    return "UNCLEAR"
```

**Our Approach** (Used here):
```python
# ‚úÖ Prompt-driven logic
SYSTEM_PROMPT = """
Accept common typos: "rok" ‚Üí rock
Accept synonyms: "stone" ‚Üí rock  
Reject multiple moves: "rock and paper" ‚Üí UNCLEAR
"""
# LLM makes the decision based on instructions
```

### System Prompt Structure (`prompts.py`)

The system prompt is the **heart of the assignment**. It contains:

1. **Game Rules**
   ```
   Valid moves: rock, paper, scissors, bomb
   Bomb beats everything, can only be used ONCE
   ```

2. **Classification Framework**
   ```
   VALID: Clear intent to play a valid move
   INVALID: Not a game move or prohibited (e.g., bomb reuse)
   UNCLEAR: Ambiguous, could mean multiple things
   ```

3. **Edge Case Instructions**
   ```
   Accept typos: "rok" ‚Üí rock, "papper" ‚Üí paper
   Accept synonyms: "stone" ‚Üí rock, "dynamite" ‚Üí bomb
   Reject multiple moves: "rock and paper" ‚Üí UNCLEAR
   Reject vague inputs: "my special move" ‚Üí UNCLEAR
   ```

4. **Constraint Enforcement**
   ```
   If user has already used bomb ‚Üí INVALID (not UNCLEAR)
   ```

5. **Output Format**
   ```
   Respond with structured JSON:
   - classification
   - interpreted_move
   - reasoning
   ```

### Context Injection

Each prompt includes game state:
```python
f"""
GAME STATE:
- Round: {round_number}
- User has used bomb: {True/False}
- Last user move: {previous_move}

USER INPUT: "{user_input}"
"""
```

This allows the LLM to enforce **stateful constraints** like "bomb only once".

---

## üéÆ How to Run

### Option 1: CLI Version (PRIMARY - Required for Assignment)

```bash
python main.py
```

**Features:**
- Simple text-based interface
- Natural language input
- Clear round-by-round feedback
- Shows reasoning for classifications
- Commands: `rules`, `score`, `quit`

**Example Session:**
```
Your move: rock
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
ROUND 1
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

You played: ROCK
Bot played: SCISSORS

Rock crushes scissors. You win!
üéâ You win this round!

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SCORE: You 1 - 0 Bot
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Your move: I want to use my bomb
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
ROUND 2
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

You played: BOMB
Bot played: ROCK

Your BOMB destroys ROCK. You win!
üéâ You win this round!

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SCORE: You 2 - 0 Bot
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Your move: bomb again
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
ROUND 3
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

You played: INVALID/UNCLEAR
Bot played: PAPER

‚ö†Ô∏è  User has already used their bomb once. Cannot use it again.

Your move was INVALID. Turn wasted. (Bot played PAPER)
‚ùå Turn wasted!

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SCORE: You 2 - 0 Bot
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

### Option 2: FastAPI + Web UI (BONUS - Not Required but Included)

```bash
# Start API server
python api.py

# Server will run on http://localhost:8000

# Then open index.html in your browser
```

**Features:**
- Beautiful visual interface
- Quick move buttons (Rock, Paper, Scissors, Bomb)
- Natural language input field
- Real-time score tracking
- Animated round results
- Automatic game-over detection

**API Documentation:**
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

**Why Include This?**
- Demonstrates production-ready skills
- Shows REST API design
- Provides better demo experience
- **But CLI is the primary submission as per assignment PDF**

---

## üß™ Edge Cases Handled

### 1. Typos (Handled via Prompt)

| Input | Interpretation | Reasoning |
|-------|---------------|-----------|
| "rok" | VALID ‚Üí rock | Common typo |
| "papper" | VALID ‚Üí paper | Common typo |
| "scisors" | VALID ‚Üí scissors | Common typo |
| "sciccors" | VALID ‚Üí scissors | Multiple typos |

**Implementation:** LLM instructed to "Accept common typos"

### 2. Synonyms (Handled via Prompt)

| Input | Interpretation | Reasoning |
|-------|---------------|-----------|
| "stone" | VALID ‚Üí rock | Synonym accepted |
| "dynamite" | VALID ‚Üí bomb | Synonym for bomb |
| "explosion" | VALID ‚Üí bomb | Synonym for bomb |
| "blast" | VALID ‚Üí bomb | Synonym for bomb |

**Implementation:** LLM instructed to "Accept synonyms: stone ‚Üí rock, dynamite ‚Üí bomb"

### 3. Ambiguous Inputs (Handled via Prompt)

| Input | Classification | Reasoning |
|-------|---------------|-----------|
| "rock and paper" | UNCLEAR | Multiple moves specified |
| "maybe rock" | UNCLEAR | Uncertain intent |
| "rock or paper" | UNCLEAR | Multiple options given |
| "both" | UNCLEAR | Ambiguous reference |

**Implementation:** LLM instructed to "Reject multiple moves ‚Üí UNCLEAR"

### 4. Vague Inputs (Handled via Prompt)

| Input | Classification | Reasoning |
|-------|---------------|-----------|
| "my special move" | UNCLEAR | Too vague |
| "the best one" | UNCLEAR | Not specific |
| "you know what I mean" | UNCLEAR | Requires context |
| "something good" | UNCLEAR | Not specific |

**Implementation:** LLM instructed to "Mark vague inputs as UNCLEAR"

### 5. Invalid Inputs (Handled via Prompt)

| Input | Classification | Reasoning |
|-------|---------------|-----------|
| "unicorn" | INVALID | Not a valid move |
| "dragon" | INVALID | Not in game |
| "fire" | INVALID | Not a valid move |
| "123" | INVALID | Nonsensical |

**Implementation:** LLM instructed to "If not in valid moves ‚Üí INVALID"

### 6. Constraint Violations (Handled via Prompt + State)

| Input | Classification | Reasoning |
|-------|---------------|-----------|
| "bomb" (1st time) | VALID ‚Üí bomb | Bomb not yet used |
| "bomb" (2nd time) | INVALID | Bomb already used |
| "dynamite" (2nd time) | INVALID | Bomb already used (synonym) |

**Implementation:** 
```python
# Game state passed to LLM
state = {
    'user_bomb_used': True  # Context for LLM
}

# Prompt includes:
"User CANNOT use bomb (already used)"
```

### 7. Natural Language (Handled via Prompt)

| Input | Interpretation | Reasoning |
|-------|---------------|-----------|
| "I choose rock" | VALID ‚Üí rock | Natural language accepted |
| "let's play scissors" | VALID ‚Üí scissors | Conversational input |
| "I'll go with paper" | VALID ‚Üí paper | Natural phrasing |
| "throw rock" | VALID ‚Üí rock | Action verb accepted |

**Implementation:** LLM trained on natural language understanding

---

## üß™ Testing

### Running Tests

```bash
# Test individual components
python judge.py           # Test intent understanding
python game_logic.py      # Test game mechanics
python response.py        # Test response formatting
```

### Manual Testing Checklist

**Test these inputs in CLI:**

‚úÖ Standard moves: `rock`, `paper`, `scissors`, `bomb`  
‚úÖ Typos: `rok`, `papper`, `scisors`  
‚úÖ Synonyms: `stone`, `dynamite`, `explosion`  
‚úÖ Natural language: `I choose rock`, `let's play scissors`  
‚úÖ Ambiguous: `rock and paper`, `maybe rock`  
‚úÖ Invalid: `unicorn`, `dragon`, `123`  
‚úÖ Bomb reuse: Play `bomb` twice in a game  
‚úÖ Commands: `rules`, `score`, `quit`  

### Expected Behavior

**Good Input:**
```
Your move: rok
ROUND 1
You played: ROCK
Bot played: SCISSORS
‚Üí You win! Rock crushes scissors.
Score: You 1 - 0 Bot
```

**Ambiguous Input:**
```
Your move: rock and paper
ROUND 2
You played: INVALID/UNCLEAR
‚ö†Ô∏è  Multiple moves specified. Please choose one.
‚Üí Turn wasted.
Score: You 1 - 0 Bot
```

**Constraint Violation:**
```
Your move: bomb
ROUND 3
You played: BOMB
‚Üí Your BOMB destroys ROCK. You win!

Your move: bomb
ROUND 4
You played: INVALID/UNCLEAR
‚ö†Ô∏è  User has already used bomb. Cannot use again.
‚Üí Turn wasted.
```

---

## üéì Design Decisions

### 1. Why Three Classifications (VALID/INVALID/UNCLEAR)?

**Decision:** Use three states instead of just VALID/INVALID

**Reasoning:**
- **VALID:** High confidence the user means a specific move
- **INVALID:** Explicitly not allowed (bomb reuse, nonsense)
- **UNCLEAR:** Ambiguous input - system refuses to guess

**Benefit:** Prevents false positives while being fair to users

**Example:**
```
"rock and paper" ‚Üí UNCLEAR (not INVALID)
Reasoning: Could mean either move, not necessarily wrong intent
```

### 2. Why LangChain + Pydantic?

**Decision:** Use structured outputs with Pydantic models

**Reasoning:**
- **Type Safety:** LLM must return valid structure
- **Validation:** Automatic schema checking
- **Clear Contracts:** Each layer knows what to expect
- **Easy Testing:** Can mock Pydantic responses

**Example:**
```python
class MoveInterpretation(BaseModel):
    classification: Literal["VALID", "INVALID", "UNCLEAR"]
    interpreted_move: Optional[Literal["rock", "paper", "scissors", "bomb"]]
    reasoning: str
    raw_input: str
```

### 3. Why Low Temperature (0.1)?

**Decision:** Use temperature=0.1 for LLM

**Reasoning:**
- **Consistency:** Same input ‚Üí same classification
- **Rule Enforcement:** Less creative, more deterministic
- **Constraint Adherence:** Follows instructions precisely

**Trade-off:** Less variation, but that's what we want for a judge

### 4. Why Context Injection?

**Decision:** Pass game state to LLM on every request

**Reasoning:**
- **Stateful Constraints:** LLM knows if bomb was used
- **History Awareness:** Can reference previous moves
- **Temporal Rules:** Enforces "only once" constraints

**Example:**
```python
prompt = f"""
User has used bomb: {True/False}
Last move: {previous_move}

USER INPUT: "bomb"
"""
# LLM can now enforce: CANNOT use bomb twice
```

### 5. Why CLI as Primary Interface?

**Decision:** CLI is primary, Web UI is bonus

**Reasoning:**
- **Assignment PDF:** "No UI" required
- **Focus:** Prompt engineering, not frontend design
- **Simplicity:** Easier to test and debug
- **Authenticity:** Shows pure AI Judge behavior

**But Web UI Included Because:**
- Better demo experience
- Shows production readiness
- Demonstrates REST API skills
- **Still, CLI is the core submission**

---

## üöÄ What Would Be Improved Next

### 1. Few-Shot Learning

**Current:** Zero-shot with instructions

**Improvement:** Add examples in system prompt
```
EXAMPLES:
Input: "rok" ‚Üí VALID, move: rock, reasoning: "Common typo"
Input: "rock and paper" ‚Üí UNCLEAR, reasoning: "Multiple moves"
```

**Benefit:** Better consistency on edge cases

### 2. Confidence Scoring

**Current:** Binary VALID/INVALID/UNCLEAR

**Improvement:** Add confidence score (0-1)
```python
class MoveInterpretation(BaseModel):
    confidence: float  # 0.0 to 1.0
```

**Use Cases:**
- Low confidence VALID ‚Üí ask user to confirm
- Track confidence over time ‚Üí detect user confusion

### 3. Explicit Chain-of-Thought

**Current:** Single-step reasoning

**Improvement:** Prompt for step-by-step reasoning
```
Step 1: Extract core term from input
Step 2: Map to valid moves or synonyms  
Step 3: Check constraints
Step 4: Classify
```

**Benefit:** More transparent, easier to debug

### 4. User Feedback Loop

**Current:** No learning from mistakes

**Improvement:**
- If user says "no, I meant X", store correction
- Build user-specific synonym dictionary
- Improve prompt with common mistakes

### 5. Multi-Language Support

**Current:** English only

**Improvement:**
- Detect input language
- Support moves in multiple languages
- "piedra" ‚Üí rock (Spanish)

### 6. Advanced Bot Strategy

**Current:** Random bot moves

**Improvement:**
- Learn from user patterns
- Strategic bomb usage
- Adaptive difficulty

### 7. Caching Common Interpretations

**Current:** Every input calls LLM

**Improvement:**
- Cache: "rock" ‚Üí always VALID
- Only call LLM for ambiguous inputs
- Reduce latency and costs

### 8. Prompt Versioning

**Current:** Single prompt

**Improvement:**
- Version prompts (v1, v2, etc.)
- A/B test variations
- Track classification accuracy

### 9. Adversarial Testing

**Current:** Basic edge cases

**Improvement:**
- Red-team testing
- Intentional confusion attempts
- Add adversarial examples to prompt

### 10. Deployment Enhancements

**Current:** Development setup

**Improvement:**
- Docker containerization
- Redis for session storage
- Rate limiting
- Monitoring and logging

---

## üõ†Ô∏è Technical Stack

### Core Dependencies

```
google-generativeai>=0.3.0    # Google Gemini API
langchain>=0.1.0              # LLM framework
langchain-google-genai>=0.0.5 # Gemini integration
pydantic>=2.0.0               # Data validation
python-dotenv>=1.0.0          # Environment variables
```

### Bonus Dependencies (for Web UI)

```
fastapi>=0.104.0              # REST API framework
uvicorn>=0.24.0               # ASGI server
```

### Why These Choices?

**LangChain:**
- Industry standard for LLM applications
- Structured output support
- Easy prompt management
- Good documentation

**Pydantic:**
- Type safety for LLM outputs
- Automatic validation
- Clear data contracts
- FastAPI integration

**Google Gemini:**
- Free tier available
- Good performance
- Structured output support
- As specified in assignment

**FastAPI (Bonus):**
- Modern Python web framework
- Automatic API documentation
- Type hints support
- Production-ready

---

## üìù Summary

### What Makes This Implementation Strong?

1. **Prompt-First Design** ‚≠ê
   - 80% of logic in prompts, not code
   - Comprehensive system prompt
   - Clear edge case instructions

2. **Clean Architecture**
   - Three distinct layers
   - Single responsibility per layer
   - Easy to test and maintain

3. **Comprehensive Edge Cases**
   - Typos, synonyms, ambiguity
   - Natural language support
   - Constraint enforcement

4. **Explainability**
   - Every decision has reasoning
   - Clear feedback to users
   - Transparent LLM decisions

5. **Production Quality**
   - Type-safe with Pydantic
   - Error handling
   - Two interfaces (CLI + API)

---

## Questions?

For any questions about the implementation, design decisions, or how to run the project, refer to:

- This README for complete documentation
- `prompts.py` for the system prompt (the heart of the project)
- Demo videos for visual walkthrough
- Code comments for specific implementation details

**Remember:** The focus is on **prompt engineering**, not complex code. The prompts drive the intelligence, the code just orchestrates it.

---

**Thank you for reviewing this submission!** üöÄ
